{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca886bf6",
   "metadata": {},
   "source": [
    "# BioSignal-X Baseline Benchmarks\n",
    "This notebook compares EfficientNet-B0 + metadata fusion vs. ViT-based fusion on ISIC/HAM10000 templates.\n",
    "Metrics: AUC, sensitivity, specificity, Brier score, ECE, and Grad-CAM visualization. Outputs results to `results/benchmark_metrics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, math, json, random, time, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score, brier_score_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append(str(Path('src').resolve()))\n",
    "from data_loader import SkinDataset, create_sample_dataset\n",
    "from models.biosignal_model import BioSignalModel\n",
    "from utils.gradcam import GradCAM\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "RESULTS_DIR = Path('results'); RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8869cb6a",
   "metadata": {},
   "source": [
    "### Data Templates (ISIC/HAM10000)\n",
    "Update the paths below to point to your local ISIC/HAM10000 datasets. For quick smoke tests, the synthetic sample dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset paths (edit as needed)\n",
    "DATASET_NAME = 'SYNTHETIC'  # options: SYNTHETIC, ISIC, HAM10000\n",
    "DATA_ROOT = Path('data')\n",
    "\n",
    "if DATASET_NAME.upper() == 'SYNTHETIC':\n",
    "    df = create_sample_dataset(n=256, out_dir=str(DATA_ROOT / 'synthetic_benchmark'))\n",
    "    from torchvision import transforms\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    ds = SkinDataset(df, img_col='image_path', label_col='label', metadata_cols=['age','sex_M','sex_F'], transform=tfm)\n",
    "else:\n",
    "    # TODO: Implement real ISIC/HAM10000 readers mapping to SkinDataset-compatible DataFrame\n",
    "    raise NotImplementedError('Provide ISIC/HAM10000 loaders and set DATASET_NAME accordingly.')\n",
    "\n",
    "# Split\n",
    "indices = np.arange(len(ds))\n",
    "np.random.seed(42); np.random.shuffle(indices)\n",
    "split = int(0.8*len(ds))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "train_loader = DataLoader(Subset(ds, train_idx), batch_size=16, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(Subset(ds, val_idx), batch_size=32, shuffle=False, num_workers=0)\n",
    "n_classes = 2\n",
    "n_meta = ds[0]['metadata'].shape[0] if isinstance(ds[0], dict) else len(ds[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef656ac4",
   "metadata": {},
   "source": [
    "### Models: EfficientNet-B0 + metadata fusion vs. ViT-based fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a9c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet-based model from project\n",
    "eff_model = BioSignalModel(num_classes=n_classes, metadata_dim=n_meta).to(DEVICE)\n",
    "\n",
    "# ViT-based fusion baseline using timm\n",
    "import timm\n",
    "class ViTFusion(nn.Module):\n",
    "    def __init__(self, num_classes, metadata_dim, vit_name='vit_base_patch16_224'):\n",
    "        super().__init__()\n",
    "        self.vit = timm.create_model(vit_name, pretrained=False, num_classes=0)\n",
    "        vis_dim = self.vit.num_features\n",
    "        self.meta = nn.Sequential(nn.Linear(metadata_dim, 64), nn.ReLU(), nn.Dropout(0.1), nn.Linear(64, 64), nn.ReLU())\n",
    "        self.head = nn.Sequential(nn.Linear(vis_dim+64, 128), nn.ReLU(), nn.Dropout(0.2), nn.Linear(128, num_classes))\n",
    "    def forward(self, x, m):\n",
    "        v = self.vit(x)\n",
    "        u = self.meta(m)\n",
    "        return self.head(torch.cat([v,u], dim=1))\n",
    "\n",
    "vit_model = ViTFusion(n_classes, n_meta).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b5db6",
   "metadata": {},
   "source": [
    "### Training/Evaluation Utilities and Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15946f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ece_score(probs, labels, n_bins=10):\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    idx = np.digitize(probs, bins) - 1\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        mask = idx == b\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        acc = (labels[mask] == (probs[mask] >= 0.5)).mean()\n",
    "        conf = probs[mask].mean()\n",
    "        ece += (np.sum(mask)/len(probs)) * abs(acc - conf)\n",
    "    return float(ece)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device=DEVICE):\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "    for batch in loader:\n",
    "        if isinstance(batch, dict):\n",
    "            x = batch['image'].to(device)\n",
    "            m = batch['metadata'].to(device, dtype=torch.float)\n",
    "            y = batch['label'].to(device)\n",
    "        else:\n",
    "            x, y, m = batch\n",
    "            x, y, m = x.to(device), y.to(device), m.to(device, dtype=torch.float)\n",
    "        logits = model(x, m)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1] if logits.shape[1] > 1 else torch.sigmoid(logits.squeeze(1))\n",
    "        y_true.append(y.detach().cpu().numpy())\n",
    "        y_prob.append(probs.detach().cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_prob = np.concatenate(y_prob)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn + 1e-8)\n",
    "    spec = tn / (tn + fp + 1e-8)\n",
    "    brier = brier_score_loss(y_true, y_prob)\n",
    "    ece = ece_score(y_prob, y_true)\n",
    "    return {\n",
    "        'auc': float(auc), 'sensitivity': float(sens), 'specificity': float(spec),\n",
    "        'brier': float(brier), 'ece': float(ece)\n",
    "    }\n",
    "\n",
    "def save_metrics(name, metrics, path=RESULTS_DIR/\"benchmark_metrics.csv\"):\n",
    "    row = {'model': name, **metrics}\n",
    "    if Path(path).exists():\n",
    "        pd.DataFrame([row]).to_csv(path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        pd.DataFrame([row]).to_csv(path, index=False)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d68176",
   "metadata": {},
   "source": [
    "### Quick Training (1 epoch) and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77fc47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, device=DEVICE, lr=1e-3):\n",
    "    model.train()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "    for x in loader:\n",
    "        if isinstance(x, dict):\n",
    "            imgs = x['image'].to(device)\n",
    "            metas = x['metadata'].to(device, dtype=torch.float)\n",
    "            labels = x['label'].to(device)\n",
    "        else:\n",
    "            imgs, labels, metas = x\n",
    "            imgs, labels, metas = imgs.to(device), labels.to(device), metas.to(device, dtype=torch.float)\n",
    "        opt.zero_grad()\n",
    "        logits = model(imgs, metas)\n",
    "        if n_classes > 1:\n",
    "            loss = criterion(logits, labels)\n",
    "        else:\n",
    "            loss = criterion(logits.squeeze(1), labels.float())\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "# Train tiny epoch\n",
    "train_one_epoch(eff_model, train_loader)\n",
    "eff_metrics = evaluate(eff_model, val_loader)\n",
    "save_metrics('efficientnet_b0_fusion', eff_metrics)\n",
    "print('EfficientNet metrics:', eff_metrics)\n",
    "\n",
    "train_one_epoch(vit_model, train_loader)\n",
    "vit_metrics = evaluate(vit_model, val_loader)\n",
    "save_metrics('vit_base_patch16_fusion', vit_metrics)\n",
    "print('ViT metrics:', vit_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27aeed",
   "metadata": {},
   "source": [
    "### Grad-CAM Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Grad-CAM on a single validation sample (EfficientNet)\n",
    "batch = next(iter(val_loader))\n",
    "if isinstance(batch, dict):\n",
    "    img = batch['image'][0:1].to(DEVICE)\n",
    "    meta = batch['metadata'][0:1].to(DEVICE, dtype=torch.float)\n",
    "else:\n",
    "    img, lbl, meta = batch\n",
    "    img, meta = img[0:1].to(DEVICE), meta[0:1].to(DEVICE, dtype=torch.float)\n",
    "\n",
    "eff_model.eval()\n",
    "cam = GradCAM(eff_model)\n",
    "cam_map = cam(img, meta)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(cam_map, cmap='jet')\n",
    "plt.title('Grad-CAM (EfficientNet Fusion)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
