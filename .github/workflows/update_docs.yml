name: Update Documentation

on:
  schedule:
    - cron: '0 2 * * 0'  # Weekly at 02:00 UTC
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      - name: Install from lock
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.lock
      - name: Lint & Static Checks
        continue-on-error: true
        run: |
          ruff check . --output-format=github
      - name: Enforce Critical Lint (F/E)
        run: |
          ruff check . --output-format=github --select F,E
      - name: Run Tests
        run: |
          python -m pytest -q

  update-docs:
    needs: validate
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      - name: Install from lock
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.lock
      - name: Generate Docs Updates
        shell: bash
        run: |
      python - <<'PY'
import os, json, re, csv, math
from datetime import datetime
from pathlib import Path

ROOT = Path('.')
DOCS = ROOT / 'docs'
DOCS.mkdir(exist_ok=True)
RESULTS = ROOT / 'results'

def replace_block(text: str, begin: str, end: str, new_content: str) -> str:
  pattern = re.compile(rf"{re.escape(begin)}.*?{re.escape(end)}", re.S)
  block = f"{begin}\n{new_content.rstrip()}\n{end}"
  if re.search(pattern, text):
    return re.sub(pattern, block, text)
  else:
    # append with two newlines
    return text.rstrip() + "\n\n" + block + "\n"

def _norm(s: str) -> str:
  return re.sub(r"[^a-z0-9]", "", s.lower())

def read_csv_metric(path: Path, candidate_columns: list[str]):
  if not path.exists():
    return None
  try:
    with path.open('r', encoding='utf-8') as f:
      reader = csv.DictReader(f)
      if not reader.fieldnames:
        return None
      fields_norm = {_norm(x): x for x in reader.fieldnames}
      col = None
      for c in candidate_columns:
        key = _norm(c)
        if key in fields_norm:
          col = fields_norm[key]
          break
      if not col:
        # try contains-based match
        for c in candidate_columns:
          key = _norm(c)
          for k in reader.fieldnames:
            if key in _norm(k):
              col = k
              break
          if col:
            break
      if not col:
        return None
      vals = []
      for row in reader:
        try:
          v = float(str(row.get(col, '')).strip().replace('%',''))
          vals.append(v)
        except Exception:
          continue
      if not vals:
        return None
      return sum(vals)/len(vals)
  except Exception:
    return None

def read_epoch_metrics(path: Path, auc_cols=None, ece_cols=None, epoch_cols=None):
  """Return list of (epoch, auc, ece) tuples if possible."""
  if auc_cols is None:
    auc_cols = ['auc','roc_auc','auroc','rocauc']
  if ece_cols is None:
    ece_cols = ['ece','expected_calibration_error']
  if epoch_cols is None:
    epoch_cols = ['epoch','step','iteration']
  if not path.exists():
    return []
  try:
    with path.open('r', encoding='utf-8') as f:
      reader = csv.DictReader(f)
      if not reader.fieldnames:
        return []
      # map normalized names
      norm_map = {_norm(fn): fn for fn in reader.fieldnames}
      def pick(cols):
        for c in cols:
          nc = _norm(c)
          if nc in norm_map:
            return norm_map[nc]
        # fuzzy contains
        for c in cols:
          nc = _norm(c)
          for k in reader.fieldnames:
            if nc in _norm(k):
              return k
        return None
      epoch_col = pick(epoch_cols)
      auc_col = pick(auc_cols)
      ece_col = pick(ece_cols)
      rows = []
      for row in reader:
        try:
          ep = int(float(row[epoch_col])) if epoch_col and row.get(epoch_col) not in (None,'') else None
        except Exception:
          ep = None
        try:
          auc_v = float(str(row[auc_col]).replace('%','')) if auc_col and row.get(auc_col) not in (None,'') else None
        except Exception:
          auc_v = None
        try:
          ece_v = float(str(row[ece_col]).replace('%','')) if ece_col and row.get(ece_col) not in (None,'') else None
        except Exception:
          ece_v = None
        if ep is not None and (auc_v is not None or ece_v is not None):
          rows.append((ep, auc_v, ece_v))
      rows.sort(key=lambda t: t[0])
      return rows
  except Exception:
    return []

def rolling_stats(values: list[float], window: int):
  vals = [v for v in values if v is not None]
  if len(vals) == 0:
    return None, None
  tail = vals[-window:] if len(vals) >= window else vals
  mean = sum(tail)/len(tail)
  if len(tail) < 2:
    return mean, None
  var = sum((x-mean)**2 for x in tail)/(len(tail)-1)
  return mean, math.sqrt(var)

def last_value(values: list[float]):
  for v in reversed(values):
    if v is not None:
      return v
  return None

def generate_sparkline_svg(values: list[float], width: int = 150, height: int = 50, stroke: str = 'green') -> str | None:
  pts = [v for v in values if v is not None]
  if len(pts) < 2:
    return None
  vmin, vmax = min(pts), max(pts)
  # avoid zero division; if flat line, center it
  if vmax - vmin < 1e-12:
    y = height/2
    path = f"M 0 {y} L {width} {y}"
    return f"<svg viewBox=\"0 0 {width} {height}\" width=\"{width}\" height=\"{height}\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"{path}\" fill=\"none\" stroke=\"{stroke}\" stroke-width=\"2\"/></svg>"
  n = len(values)
  # compute points; skip None by carrying last known
  coords = []
  last_y = None
  for i, v in enumerate(values):
    x = (i/(n-1)) * (width-1)
    if v is None:
      if last_y is None:
        continue
      y = last_y
    else:
      y = height - ((v - vmin) / (vmax - vmin)) * (height-1)
      last_y = y
    coords.append((x, y))
  if len(coords) < 2:
    return None
  d = " ".join([("M" if idx == 0 else "L") + f" {x:.1f} {y:.1f}" for idx, (x, y) in enumerate(coords)])
  return f"<svg viewBox=\"0 0 {width} {height}\" width=\"{width}\" height=\"{height}\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"{d}\" fill=\"none\" stroke=\"{stroke}\" stroke-width=\"2\"/></svg>"

def find_in_obj(obj, candidate_keys: list[str]):
  # Depth-first search for first numeric match by key name
  def _search(o):
    if isinstance(o, dict):
      # direct key hit
      for k, v in o.items():
        nk = _norm(k)
        if nk in {_norm(c) for c in candidate_keys} or any(_norm(c) in nk for c in candidate_keys):
          try:
            return float(str(v).replace('%',''))
          except Exception:
            pass
      # recurse
      for v in o.values():
        r = _search(v)
        if r is not None:
          return r
    elif isinstance(o, list):
      for it in o:
        r = _search(it)
        if r is not None:
          return r
    return None
  return _search(obj)

def fmt_num(x, decimals=3):
  try:
    return f"{float(x):.{decimals}f}"
  except Exception:
    return "N/A"

def fmt_pct(x):
  try:
    x = float(x)
    if x <= 1.0:
      x = x * 100.0
    return f"{x:.1f}%"
  except Exception:
    return "N/A"

# --- Compliance Guidelines ---
cg = DOCS / 'compliance_guidelines.md'
base_cg = cg.read_text(encoding='utf-8') if cg.exists() else "# Compliance Guidelines\n"
ci_block = [
  f"- Last run: {datetime.utcnow().isoformat()}Z",
  "- Tests: see Actions summary",
  "- Lint: Ruff (F/E errors block, W/B/I warn)",
]
fs = RESULTS / 'fairness_summary.json'
eval_lines = []
if fs.exists():
  try:
    data = json.loads(fs.read_text(encoding='utf-8'))
    eval_lines.append(f"- Fairness groups: {len(data.get('aggregates', []))}")
    eval_lines.append(f"- ΔAUC (max): {data.get('delta_auc')}")
    eval_lines.append(f"- ΔECE (max): {data.get('delta_ece')}")
  except Exception:
    eval_lines.append("- Fairness summary not parsable.")
else:
  eval_lines.append("- No fairness summary found.")

base_cg = replace_block(base_cg, "<!-- CI_SUMMARY:BEGIN -->", "<!-- CI_SUMMARY:END -->", "\n".join(ci_block))
base_cg = replace_block(base_cg, "<!-- EVAL_SUMMARY:BEGIN -->", "<!-- EVAL_SUMMARY:END -->", "\n".join(eval_lines))

# Model performance indicators
calib_csv = RESULTS / 'calibration_report.csv'
bench_csv = RESULTS / 'benchmark_metrics.csv'
drift_json = RESULTS / 'drift_report.json'

auc = read_csv_metric(bench_csv, ['auc', 'roc_auc', 'auroc', 'rocauc'])
ece = read_csv_metric(calib_csv, ['ece', 'expected_calibration_error'])
drift_rate = None
if drift_json.exists():
  try:
    j = json.loads(drift_json.read_text(encoding='utf-8'))
    drift_rate = find_in_obj(j, ['drift_rate','driftrate','drift_rate_pct','drift_detected_rate','rate'])
  except Exception:
    drift_rate = None

last_run = f"Last run: {datetime.utcnow().isoformat()}Z"
mp_lines = [
  last_run,
  "",
  "| Metric | Value |",
  "|---|---|",
  f"| AUC | {fmt_num(auc) if auc is not None else 'N/A'} |",
  f"| ECE | {fmt_num(ece) if ece is not None else 'N/A'} |",
  f"| Drift rate | {fmt_pct(drift_rate) if drift_rate is not None else 'N/A'} |",
]
base_cg = replace_block(base_cg, "<!-- MODEL_PERFORMANCE:BEGIN -->", "<!-- MODEL_PERFORMANCE:END -->", "\n".join(mp_lines))

# Uncertainty + Fairness summary for compliance
unc = read_csv_metric(calib_csv, [
  'predictive_entropy_mean','mean_uncertainty','avg_uncertainty',
  'uncertainty','entropy','predictive_entropy'
])

def read_fairness_deltas() -> tuple:
  candidates = [RESULTS / 'fairness' / 'fairness_summary.json', RESULTS / 'fairness_summary.json']
  for p in candidates:
    if p.exists():
      try:
        d = json.loads(p.read_text(encoding='utf-8'))
        da = d.get('delta_auc')
        de = d.get('delta_ece')
        # Sometimes strings
        da = float(str(da)) if da is not None else None
        de = float(str(de)) if de is not None else None
        return da, de
      except Exception:
        continue
  return None, None

delta_auc, delta_ece = read_fairness_deltas()
unc_lines = [
  "| Metric | Value |",
  "|---|---|",
  f"| Mean uncertainty | {fmt_num(unc, 2) if unc is not None else 'N/A'} |",
  f"| ΔAUC | {fmt_num(delta_auc, 2) if delta_auc is not None else 'N/A'} |",
  f"| ΔECE | {fmt_num(delta_ece, 2) if delta_ece is not None else 'N/A'} |",
  f"| Last updated | {datetime.utcnow().isoformat()}Z |",
]
base_cg = replace_block(base_cg, "<!-- UNCERTAINTY_SUMMARY:BEGIN -->", "<!-- UNCERTAINTY_SUMMARY:END -->", "\n".join(unc_lines))

# Temporal stability (rolling stats)
epoch_rows = read_epoch_metrics(calib_csv)
auc_series = [r[1] for r in epoch_rows if r[1] is not None]
ece_series = [r[2] for r in epoch_rows if r[2] is not None]
last_epoch = epoch_rows[-1][0] if epoch_rows else None
auc_mean, auc_std = rolling_stats(auc_series, 5)
ece_mean, ece_std = rolling_stats(ece_series, 5)
ts_lines = [
  "| Metric | Mean | Std Dev | Last Epoch |",
  "|---|---|---|---|",
  f"| AUC | {fmt_num(auc_mean,3) if auc_mean is not None else 'N/A'} | {fmt_num(auc_std,3) if auc_std is not None else 'N/A'} | {last_epoch if last_epoch is not None else 'N/A'} |",
  f"| ECE | {fmt_num(ece_mean,3) if ece_mean is not None else 'N/A'} | {fmt_num(ece_std,3) if ece_std is not None else 'N/A'} | {last_epoch if last_epoch is not None else 'N/A'} |",
  f"| Updated | {datetime.utcnow().isoformat()}Z |  |  |",
]
auc_svg = generate_sparkline_svg(auc_series, stroke="#10b981")  # green
ece_svg = generate_sparkline_svg(ece_series, stroke="#f59e0b")  # orange
auc_last = last_value(auc_series)
ece_last = last_value(ece_series)
ts_extra = []
if auc_svg or ece_svg:
  ts_extra.append("")
  ts_extra.append("AUC trend:")
  ts_extra.append(auc_svg if auc_svg else "No trend data available")
  ts_extra.append("")
  ts_extra.append("ECE trend:")
  ts_extra.append(ece_svg if ece_svg else "No trend data available")
else:
  ts_extra.append("")
  ts_extra.append("No trend data available")
ts_extra.append("")
ts_extra.append(
  f"Averages — AUC mean: {fmt_num(auc_mean,3) if auc_mean is not None else 'N/A'}, last: {fmt_num(auc_last,3) if auc_last is not None else 'N/A'}; "
  f"ECE mean: {fmt_num(ece_mean,3) if ece_mean is not None else 'N/A'}, last: {fmt_num(ece_last,3) if ece_last is not None else 'N/A'}"
)
ts_content = "\n".join(ts_lines + ts_extra)
base_cg = replace_block(base_cg, "<!-- TEMPORAL_STABILITY:BEGIN -->", "<!-- TEMPORAL_STABILITY:END -->", ts_content)
cg.write_text(base_cg, encoding='utf-8')

# --- README.md ---
rd = ROOT / 'README.md'
readme = rd.read_text(encoding='utf-8') if rd.exists() else "# BioSignal-AI\n"
badge_md = (
  "[![Tests](https://github.com/${{ github.repository }}/actions/workflows/tests.yml/badge.svg)](https://github.com/${{ github.repository }}/actions/workflows/tests.yml) "
  "[![Nightly Report](https://github.com/${{ github.repository }}/actions/workflows/publish_eval_report.yml/badge.svg)](https://github.com/${{ github.repository }}/actions/workflows/publish_eval_report.yml) "
  "[![Docs Sync](https://github.com/${{ github.repository }}/actions/workflows/update_docs.yml/badge.svg)](https://github.com/${{ github.repository }}/actions/workflows/update_docs.yml)"
)
readme = replace_block(readme, "<!-- CI_BADGES:BEGIN -->", "<!-- CI_BADGES:END -->", badge_md)
nightly = RESULTS / 'report_summary.html'
nightly_line = (
  f"Latest nightly report generated at: {datetime.utcnow().isoformat()}Z (see results/report_summary.html)"
  if nightly.exists() else "Latest nightly report not found." )
readme = replace_block(readme, "<!-- NIGHTLY_STATUS:BEGIN -->", "<!-- NIGHTLY_STATUS:END -->", nightly_line)

# Metrics summary for README
ms_lines = mp_lines  # reuse same structure
readme = replace_block(readme, "<!-- METRICS_SUMMARY:BEGIN -->", "<!-- METRICS_SUMMARY:END -->", "\n".join(ms_lines))

# Fairness summary for README
fair_lines = [
  "| Metric | Value |",
  "|---|---|",
  f"| Mean uncertainty | {fmt_num(unc, 2) if unc is not None else 'N/A'} |",
  f"| ΔAUC | {fmt_num(delta_auc, 2) if delta_auc is not None else 'N/A'} |",
  f"| ΔECE | {fmt_num(delta_ece, 2) if delta_ece is not None else 'N/A'} |",
  f"| Last updated | {datetime.utcnow().isoformat()}Z |",
]
readme = replace_block(readme, "<!-- FAIRNESS_SUMMARY:BEGIN -->", "<!-- FAIRNESS_SUMMARY:END -->", "\n".join(fair_lines))

# Performance trend (README)
pt_lines = [
  "| Metric | Mean | Std Dev | Last Epoch |",
  "|---|---|---|---|",
  f"| AUC | {fmt_num(auc_mean,3) if auc_mean is not None else 'N/A'} | {fmt_num(auc_std,3) if auc_std is not None else 'N/A'} | {last_epoch if last_epoch is not None else 'N/A'} |",
  f"| ECE | {fmt_num(ece_mean,3) if ece_mean is not None else 'N/A'} | {fmt_num(ece_std,3) if ece_std is not None else 'N/A'} | {last_epoch if last_epoch is not None else 'N/A'} |",
  f"| Updated | {datetime.utcnow().isoformat()}Z |  |  |",
]
pt_extra = []
if auc_svg or ece_svg:
  pt_extra.append("")
  pt_extra.append("AUC trend:")
  pt_extra.append(auc_svg if auc_svg else "No trend data available")
  pt_extra.append("")
  pt_extra.append("ECE trend:")
  pt_extra.append(ece_svg if ece_svg else "No trend data available")
else:
  pt_extra.append("")
  pt_extra.append("No trend data available")
pt_extra.append("")
pt_extra.append(
  f"Averages — AUC mean: {fmt_num(auc_mean,3) if auc_mean is not None else 'N/A'}, last: {fmt_num(auc_last,3) if auc_last is not None else 'N/A'}; "
  f"ECE mean: {fmt_num(ece_mean,3) if ece_mean is not None else 'N/A'}, last: {fmt_num(ece_last,3) if ece_last is not None else 'N/A'}"
)
pt_content = "\n".join(pt_lines + pt_extra)
readme = replace_block(readme, "<!-- PERFORMANCE_TREND:BEGIN -->", "<!-- PERFORMANCE_TREND:END -->", pt_content)
rd.write_text(readme, encoding='utf-8')

# --- literature_summary.md ---
lit = DOCS / 'literature_summary.md'
lit_text = lit.read_text(encoding='utf-8') if lit.exists() else "Literature Summary\n"
validation_line = f"Last validation date: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}"
lit_text = replace_block(lit_text, "<!-- VALIDATION_DATE:BEGIN -->", "<!-- VALIDATION_DATE:END -->", validation_line)
lit.write_text(lit_text, encoding='utf-8')

print('Docs updated via block replacement.')
PY
      - name: Commit changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "docs: auto-sync compliance and status reports"
          file_pattern: |
            docs/compliance_guidelines.md
            README.md
            docs/literature_summary.md
