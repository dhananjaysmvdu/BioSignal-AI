name: Update Documentation

on:
  schedule:
    - cron: '0 2 * * 0'  # Weekly at 02:00 UTC
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      - name: Install from lock
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.lock
      - name: Lint & Static Checks
        continue-on-error: true
        run: |
          ruff check . --output-format=github
      - name: Enforce Critical Lint (F/E)
        run: |
          ruff check . --output-format=github --select F,E
      - name: Run Tests
        run: |
          python -m pytest -q

  update-docs:
    needs: validate
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      - name: Install from lock
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.lock
      - name: Generate Docs Updates
        shell: bash
        run: |
      python - <<'PY'
import os, json, re, csv
from datetime import datetime
from pathlib import Path

ROOT = Path('.')
DOCS = ROOT / 'docs'
DOCS.mkdir(exist_ok=True)
RESULTS = ROOT / 'results'

def replace_block(text: str, begin: str, end: str, new_content: str) -> str:
  pattern = re.compile(rf"{re.escape(begin)}.*?{re.escape(end)}", re.S)
  block = f"{begin}\n{new_content.rstrip()}\n{end}"
  if re.search(pattern, text):
    return re.sub(pattern, block, text)
  else:
    # append with two newlines
    return text.rstrip() + "\n\n" + block + "\n"

def _norm(s: str) -> str:
  return re.sub(r"[^a-z0-9]", "", s.lower())

def read_csv_metric(path: Path, candidate_columns: list[str]):
  if not path.exists():
    return None
  try:
    with path.open('r', encoding='utf-8') as f:
      reader = csv.DictReader(f)
      if not reader.fieldnames:
        return None
      fields_norm = {_norm(x): x for x in reader.fieldnames}
      col = None
      for c in candidate_columns:
        key = _norm(c)
        if key in fields_norm:
          col = fields_norm[key]
          break
      if not col:
        # try contains-based match
        for c in candidate_columns:
          key = _norm(c)
          for k in reader.fieldnames:
            if key in _norm(k):
              col = k
              break
          if col:
            break
      if not col:
        return None
      vals = []
      for row in reader:
        try:
          v = float(str(row.get(col, '')).strip().replace('%',''))
          vals.append(v)
        except Exception:
          continue
      if not vals:
        return None
      return sum(vals)/len(vals)
  except Exception:
    return None

def find_in_obj(obj, candidate_keys: list[str]):
  # Depth-first search for first numeric match by key name
  def _search(o):
    if isinstance(o, dict):
      # direct key hit
      for k, v in o.items():
        nk = _norm(k)
        if nk in {_norm(c) for c in candidate_keys} or any(_norm(c) in nk for c in candidate_keys):
          try:
            return float(str(v).replace('%',''))
          except Exception:
            pass
      # recurse
      for v in o.values():
        r = _search(v)
        if r is not None:
          return r
    elif isinstance(o, list):
      for it in o:
        r = _search(it)
        if r is not None:
          return r
    return None
  return _search(obj)

def fmt_num(x, decimals=3):
  try:
    return f"{float(x):.{decimals}f}"
  except Exception:
    return "N/A"

def fmt_pct(x):
  try:
    x = float(x)
    if x <= 1.0:
      x = x * 100.0
    return f"{x:.1f}%"
  except Exception:
    return "N/A"

# --- Compliance Guidelines ---
cg = DOCS / 'compliance_guidelines.md'
base_cg = cg.read_text(encoding='utf-8') if cg.exists() else "# Compliance Guidelines\n"
ci_block = [
  f"- Last run: {datetime.utcnow().isoformat()}Z",
  "- Tests: see Actions summary",
  "- Lint: Ruff (F/E errors block, W/B/I warn)",
]
fs = RESULTS / 'fairness_summary.json'
eval_lines = []
if fs.exists():
  try:
    data = json.loads(fs.read_text(encoding='utf-8'))
    eval_lines.append(f"- Fairness groups: {len(data.get('aggregates', []))}")
    eval_lines.append(f"- ΔAUC (max): {data.get('delta_auc')}")
    eval_lines.append(f"- ΔECE (max): {data.get('delta_ece')}")
  except Exception:
    eval_lines.append("- Fairness summary not parsable.")
else:
  eval_lines.append("- No fairness summary found.")

base_cg = replace_block(base_cg, "<!-- CI_SUMMARY:BEGIN -->", "<!-- CI_SUMMARY:END -->", "\n".join(ci_block))
base_cg = replace_block(base_cg, "<!-- EVAL_SUMMARY:BEGIN -->", "<!-- EVAL_SUMMARY:END -->", "\n".join(eval_lines))

# Model performance indicators
calib_csv = RESULTS / 'calibration_report.csv'
bench_csv = RESULTS / 'benchmark_metrics.csv'
drift_json = RESULTS / 'drift_report.json'

auc = read_csv_metric(bench_csv, ['auc', 'roc_auc', 'auroc', 'rocauc'])
ece = read_csv_metric(calib_csv, ['ece', 'expected_calibration_error'])
drift_rate = None
if drift_json.exists():
  try:
    j = json.loads(drift_json.read_text(encoding='utf-8'))
    drift_rate = find_in_obj(j, ['drift_rate','driftrate','drift_rate_pct','drift_detected_rate','rate'])
  except Exception:
    drift_rate = None

last_run = f"Last run: {datetime.utcnow().isoformat()}Z"
mp_lines = [
  last_run,
  "",
  "| Metric | Value |",
  "|---|---|",
  f"| AUC | {fmt_num(auc) if auc is not None else 'N/A'} |",
  f"| ECE | {fmt_num(ece) if ece is not None else 'N/A'} |",
  f"| Drift rate | {fmt_pct(drift_rate) if drift_rate is not None else 'N/A'} |",
]
base_cg = replace_block(base_cg, "<!-- MODEL_PERFORMANCE:BEGIN -->", "<!-- MODEL_PERFORMANCE:END -->", "\n".join(mp_lines))

# Uncertainty + Fairness summary for compliance
unc = read_csv_metric(calib_csv, [
  'predictive_entropy_mean','mean_uncertainty','avg_uncertainty',
  'uncertainty','entropy','predictive_entropy'
])

def read_fairness_deltas() -> tuple:
  candidates = [RESULTS / 'fairness' / 'fairness_summary.json', RESULTS / 'fairness_summary.json']
  for p in candidates:
    if p.exists():
      try:
        d = json.loads(p.read_text(encoding='utf-8'))
        da = d.get('delta_auc')
        de = d.get('delta_ece')
        # Sometimes strings
        da = float(str(da)) if da is not None else None
        de = float(str(de)) if de is not None else None
        return da, de
      except Exception:
        continue
  return None, None

delta_auc, delta_ece = read_fairness_deltas()
unc_lines = [
  "| Metric | Value |",
  "|---|---|",
  f"| Mean uncertainty | {fmt_num(unc, 2) if unc is not None else 'N/A'} |",
  f"| ΔAUC | {fmt_num(delta_auc, 2) if delta_auc is not None else 'N/A'} |",
  f"| ΔECE | {fmt_num(delta_ece, 2) if delta_ece is not None else 'N/A'} |",
  f"| Last updated | {datetime.utcnow().isoformat()}Z |",
]
base_cg = replace_block(base_cg, "<!-- UNCERTAINTY_SUMMARY:BEGIN -->", "<!-- UNCERTAINTY_SUMMARY:END -->", "\n".join(unc_lines))
cg.write_text(base_cg, encoding='utf-8')

# --- README.md ---
rd = ROOT / 'README.md'
readme = rd.read_text(encoding='utf-8') if rd.exists() else "# BioSignal-AI\n"
badge_md = (
  "[![Tests](https://github.com/${{ github.repository }}/actions/workflows/tests.yml/badge.svg)](https://github.com/${{ github.repository }}/actions/workflows/tests.yml) "
  "[![Nightly Report](https://github.com/${{ github.repository }}/actions/workflows/publish_eval_report.yml/badge.svg)](https://github.com/${{ github.repository }}/actions/workflows/publish_eval_report.yml) "
  "[![Docs Sync](https://github.com/${{ github.repository }}/actions/workflows/update_docs.yml/badge.svg)](https://github.com/${{ github.repository }}/actions/workflows/update_docs.yml)"
)
readme = replace_block(readme, "<!-- CI_BADGES:BEGIN -->", "<!-- CI_BADGES:END -->", badge_md)
nightly = RESULTS / 'report_summary.html'
nightly_line = (
  f"Latest nightly report generated at: {datetime.utcnow().isoformat()}Z (see results/report_summary.html)"
  if nightly.exists() else "Latest nightly report not found." )
readme = replace_block(readme, "<!-- NIGHTLY_STATUS:BEGIN -->", "<!-- NIGHTLY_STATUS:END -->", nightly_line)

# Metrics summary for README
ms_lines = mp_lines  # reuse same structure
readme = replace_block(readme, "<!-- METRICS_SUMMARY:BEGIN -->", "<!-- METRICS_SUMMARY:END -->", "\n".join(ms_lines))

# Fairness summary for README
fair_lines = [
  "| Metric | Value |",
  "|---|---|",
  f"| Mean uncertainty | {fmt_num(unc, 2) if unc is not None else 'N/A'} |",
  f"| ΔAUC | {fmt_num(delta_auc, 2) if delta_auc is not None else 'N/A'} |",
  f"| ΔECE | {fmt_num(delta_ece, 2) if delta_ece is not None else 'N/A'} |",
  f"| Last updated | {datetime.utcnow().isoformat()}Z |",
]
readme = replace_block(readme, "<!-- FAIRNESS_SUMMARY:BEGIN -->", "<!-- FAIRNESS_SUMMARY:END -->", "\n".join(fair_lines))
rd.write_text(readme, encoding='utf-8')

# --- literature_summary.md ---
lit = DOCS / 'literature_summary.md'
lit_text = lit.read_text(encoding='utf-8') if lit.exists() else "Literature Summary\n"
validation_line = f"Last validation date: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}"
lit_text = replace_block(lit_text, "<!-- VALIDATION_DATE:BEGIN -->", "<!-- VALIDATION_DATE:END -->", validation_line)
lit.write_text(lit_text, encoding='utf-8')

print('Docs updated via block replacement.')
PY
      - name: Commit changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "docs: auto-sync compliance and status reports"
          file_pattern: |
            docs/compliance_guidelines.md
            README.md
            docs/literature_summary.md
